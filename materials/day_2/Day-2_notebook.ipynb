{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "Collaboratory workshop, 02/21/2018\n",
    "\n",
    "This is a notebook developed throughout the first day of the Collaboratory Workshop, Machine Learning with Python. For more information, go to the workshop home page:\n",
    "\n",
    "https://github.com/QCB-Collaboratory/W17.MachineLearning/wiki/Day-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download this data in our [wiki](https://github.com/QCB-Collaboratory/W17.MachineLearning/wiki/Day-2), or using this [direct link](https://github.com/QCB-Collaboratory/W17.MachineLearning/raw/master/materials/day_2/Day2_testdataset.zip). After downloading, move the file to the same place where you are running your notebook and unzip it. You should now have two files: \n",
    "\n",
    "* ```CollML_testdataset_features.dat```: contains feature values for each sample\n",
    "* ```CollML_testdataset_labels.dat```: contains the class of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_origin = np.loadtxt('CollML_testdataset_features.dat')\n",
    "labels_origin = np.loadtxt('CollML_testdataset_labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of features\", features_origin.shape)\n",
    "print(\"Shape of labels\", labels_origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_origin)   ## shows unique values in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a classifier that reproduces the labels in the NumPy array _labels_ based on _features_. Based on the shapes shown above, we have 2 features, and 500 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing/Training datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features, features_test, labels, labels_test = train_test_split( \n",
    "                features_origin, labels_origin, test_size=0.2,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the whole dataset\",features_origin.shape)\n",
    "print(\"Shape of the train dataset\",features.shape)\n",
    "print(\"Shape of the test dataset\",features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's start by visualizing this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( features[ labels == 0, 0 ], features[ labels == 0, 1 ], 'bo'  )\n",
    "plt.plot( features[ labels == 1, 0 ], features[ labels == 1, 1 ], 'rs'  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-doing the previous plot, but with more details\n",
    "plt.figure( figsize=(4,3) )\n",
    "plt.plot( features[ labels == 0, 0 ], features[ labels == 0, 1 ], 'o',\n",
    "           markersize=5, color='b')\n",
    "plt.plot( features[ labels == 1, 0 ], features[ labels == 1, 1 ], 's',\n",
    "           markersize=4, color='r')\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('SyntheticDataset.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees  - Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start creating our Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable _clf_ will contain all information learned by the classifier. To perform the learning step, we use the method _fit_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit( features, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the accuracy of this classifier, we can use the method _score_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score( features, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that our classifier was able to perfectly reproduce all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz( clf, 'Graph_DecisionTree_testdataset.dat' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature ranking by Gini Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, we know that the point $P_1 = [-1,-1]$ should be of class 0 (zero), and the point $P_2 = [-1,4]$ should belong to class 1. Let's Check the classifier's prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.array([[-1,-1]])\n",
    "P2 = np.array([[-1,4]])\n",
    "\n",
    "print(\"Prediction for P1: \", clf.predict(P1))\n",
    "print(\"Prediction for P2: \", clf.predict(P2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why did we use [[ and ]] in the previous cell?\n",
    "\n",
    "You can also check the probability that a given point belongs to a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability for P1: \", clf.predict_proba(P1))\n",
    "print(\"Probability for P2: \", clf.predict_proba(P2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's investigate the \"decision boundaries\" -- i.e. the boundary between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.5\n",
    "x     = np.arange(-2.0, 5.001, delta)\n",
    "y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z    = clf.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "Z    = Z.reshape( X.shape )\n",
    "\n",
    "plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "x     = np.arange(-2.0, 5.001, delta)\n",
    "y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z    = clf.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "Z    = Z.reshape( X.shape )\n",
    "\n",
    "plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some of the meta-parameters available in SK-learn to modify the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier( max_depth = 5 )\n",
    "clf.fit( features, labels )\n",
    "print( clf.score( features, labels ) )\n",
    "export_graphviz( clf, 'Graph_DecisionTree_testdataset_2.dat' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "x     = np.arange(-2.0, 5.001, delta)\n",
    "y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z    = clf.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "Z    = Z.reshape( X.shape )\n",
    "\n",
    "plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "plt.xlim(-2, 5)\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "Random forests are among the most important models in Machine Learning, especially for applications that demand low latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example of sampled data:\n",
    "np.random.seed(0)  # the random seed, to be sure that you always plot the same thing\n",
    "\n",
    "for i in range (3):\n",
    "    plt.figure( figsize=(4,3) )\n",
    "    \n",
    "    choice = np.random.random(size=len(labels))>0.8\n",
    "    plt.plot( features[ np.logical_and(choice , labels == 0), 0 ], features[ np.logical_and(choice , labels == 0), 1 ], 'o',\n",
    "               markersize=4, color='b')\n",
    "    plt.plot( features[ np.logical_and(choice , labels == 1), 0 ], features[ np.logical_and(choice , labels == 1), 1 ], 's',\n",
    "               markersize=4, color='r')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier( n_estimators = 50, \n",
    "                            max_depth =5, oob_score = True )\n",
    "clf.fit( features, labels )\n",
    "print(\"Out of bag score\",clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \",clf.score(features,labels))\n",
    "print(\"Features importance: \",clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction for P1: \", clf.predict(P1))\n",
    "print(\"Prediction for P2: \", clf.predict(P2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "x     = np.arange(-2.0, 5.001, delta)\n",
    "y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z    = clf.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "Z    = Z.reshape( X.shape )\n",
    "\n",
    "plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "plt.xlim(-2, 5)\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier( n_estimators = 50, \n",
    "                        max_depth =5, oob_score = True )\n",
    "clf.fit( features, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \",\n",
    "          clf.score(features,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels, clf.predict(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because random forests are ensembles of decision trees, there is a way to accesss the trees and inspect them closely. To do so, use the attribute ```estimators_``` of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how to visualize trees in the random forest model we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz( clf.estimators_[0], \n",
    "                'Graph_DecisionTree_testdataset_RF0.dat' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(learning_rate=1.5, n_estimators=100)\n",
    "clf_gb.fit( features, labels )\n",
    "clf_gb.score(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction for P1: \", clf_gb.predict(P1))\n",
    "print(\"Prediction for P2: \", clf_gb.predict(P2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "x     = np.arange(-2.0, 5.001, delta)\n",
    "y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z    = clf_gb.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "Z    = Z.reshape( X.shape )\n",
    "\n",
    "plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "plt.xlim(-2, 5)\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (or SVMs) were for a long time the most widely used model in the Machine Learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit( features, labels )\n",
    "print( clf.score( features, labels ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma=10000.)\n",
    "clf.fit( features, labels )\n",
    "print( clf.score( features, labels ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that automatically draws the decision boundaries for us (this avoids too much replication of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotContours(clf, figname, delta = 0.01):\n",
    "    \n",
    "    x     = np.arange(-2.0, 5.001, delta)\n",
    "    y     = np.arange(-2.0, 5.001, delta)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z    = clf.predict( np.c_[X.ravel(), Y.ravel()] )\n",
    "    Z    = Z.reshape( X.shape )\n",
    "\n",
    "    plt.contourf( X, Y, Z, cmap=plt.get_cmap('jet'))\n",
    "\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundary1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 1000., gamma = 100. )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundaryC100g100.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 1000 ,gamma = 1)\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundary_C1000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.02 ,gamma = 1 )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundaryC01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.5, gamma = 70. )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundaryC01g100.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.1, gamma = 1. )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundaryC001g1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.1, gamma = 0.1 )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundaryC001g01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 1, gamma = 0.01 )\n",
    "clf.fit( features, labels )\n",
    "plotContours(clf, 'SVC_decisionboundary1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split( \n",
    "        features, labels, test_size=0.33,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the train dataset: \", X_train.shape)\n",
    "print(\"Shape of the validation dataset: \", X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.001, gamma = 10. )\n",
    "clf.fit( X_train, Y_train )\n",
    "clf.score(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setGammas = np.array( [0.003,0.01,0.03,0.1,0.3,1.0,3.,10.,30.,100.,300] )\n",
    "\n",
    "accuracies = []\n",
    "for gamma in setGammas:\n",
    "    clf = SVC( C = 1., gamma = gamma )\n",
    "    clf.fit( X_train, Y_train )\n",
    "    accuracies.append( clf.score(X_valid, Y_valid) )\n",
    "\n",
    "plt.plot(setGammas, accuracies)\n",
    "\n",
    "plt.ylabel(r'Accuracy')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is the splitting is performed at random, to properly estimate accuracy you should perform the train-test splitting several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRepetitions = 200\n",
    "setGammas = np.array( [0.003,0.01,0.03,0.1,0.3,1.0,3.,10.,30.,100.,300] )\n",
    "\n",
    "accuracies = np.zeros( setGammas.shape )\n",
    "\n",
    "for j in range(numRepetitions):\n",
    "    k = 0\n",
    "    for gamma in setGammas:\n",
    "        clf = SVC( C = 1., gamma = gamma )\n",
    "        accuracies[k] += clf.fit( X_train, Y_train ).score(X_valid, Y_valid)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "accuracies = accuracies / numRepetitions\n",
    "plt.plot(setGammas, accuracies)\n",
    "\n",
    "plt.ylabel(r'Accuracy')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRepetitions = 100\n",
    "accuracies = np.zeros( numRepetitions )\n",
    "\n",
    "for j in range(numRepetitions):\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split( features, labels, \n",
    "                                                            test_size=0.33)\n",
    "    clf = SVC( C = 1., gamma = 0.5 )\n",
    "    clf.fit( X_train, Y_train )\n",
    "    accuracies[j] = clf.score(X_valid, Y_valid)\n",
    "\n",
    "print(r\"Average accuracy (gamma = 0.5): \", accuracies.mean() )\n",
    "\n",
    "\n",
    "for j in range(numRepetitions):\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split( features, labels,\n",
    "                                                            test_size=0.33)\n",
    "    clf = SVC( C = 1., gamma = 100. )\n",
    "    clf.fit( X_train, Y_train )\n",
    "    accuracies[j] = clf.score(X_valid, Y_valid)\n",
    "\n",
    "print(\"Average accuracy (gamma = 100.): \", accuracies.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "\n",
    "for train_index, valid_index in kf.split( features ):\n",
    "    X_train = features[train_index]\n",
    "    X_valid  = features[valid_index]\n",
    "    Y_train = labels[train_index]\n",
    "    Y_valid  = labels[valid_index]\n",
    "    \n",
    "    clf = SVC( C = 1., gamma = 0.5 )\n",
    "    clf.fit( X_train, Y_train )\n",
    "    print( clf.score(X_valid, Y_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train = features[train_index]\n",
    "    X_valid  = features[valid_index]\n",
    "    Y_train = labels[train_index]\n",
    "    Y_valid  = labels[valid_index]\n",
    "    \n",
    "    clf = SVC( C = 1., gamma = 0.5 )\n",
    "    clf.fit( X_train, Y_train )\n",
    "    print( clf.score(X_valid, Y_valid) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier( n_estimators = 50 )\n",
    "clf.fit( features, labels )\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(labels_test, clf.predict(features_test)))\n",
    "print(\"Precision: \", precision_score(labels_test, clf.predict(features_test)))\n",
    "print(\"Recall: \", recall_score(labels_test, clf.predict(features_test)))\n",
    "print(\"F1-score: \", f1_score(labels_test, clf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 1., gamma = 1.)\n",
    "clf.fit( features, labels )        \n",
    "        \n",
    "print(\"Accuracy: \", accuracy_score(labels_test, clf.predict(features_test)))\n",
    "print(\"Precision: \", precision_score(labels_test, clf.predict(features_test)))\n",
    "print(\"Recall: \", recall_score(labels_test, clf.predict(features_test)))\n",
    "print(\"F1-score: \", f1_score(labels_test, clf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "\n",
    "# After-the-class Practicing\n",
    "\n",
    "<br />\n",
    "\n",
    "Next, I show possible solutions to the proposed practice at the end of our slides. It is highly recommended that you try the exercises by yourself first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsing Breast Cancer dataset\n",
    "\n",
    "As an exercise, let's try and reproduce the same analysis in the same dataset we explored yesterday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bcancer = load_breast_cancer()\n",
    "\n",
    "print(\"Num samples x Num Features: \", bcancer.data.shape)\n",
    "print(\"Num samples x Num Features: \", bcancer.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bcancer = RandomForestClassifier()\n",
    "clf_bcancer.fit( bcancer.data, bcancer.target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bcancer.score( bcancer.data, bcancer.target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bcancer.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening the $\\alpha$ and $C$ meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setGammas = np.linspace(0.005,10.0,50)\n",
    "\n",
    "accuracies = []\n",
    "for gamma in setGammas:\n",
    "    clf = SVC( C = 0.001, gamma = gamma )\n",
    "    accuracies.append( clf.fit( features, labels ).score(features, labels) )\n",
    "\n",
    "plt.plot(setGammas, accuracies)\n",
    "\n",
    "plt.ylabel(r'Accuracy')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.savefig('SVM_accuracyvsgamma.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banana dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdataset = np.loadtxt('banana_dataset.csv', delimiter=',')\n",
    "print(\"Shape of the bdataset: \", bdataset.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfeat = bdataset[:,1:]\n",
    "blabl = bdataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 0.001, gamma = gamma )\n",
    "clf.fit( bfeat, blabl )\n",
    "clf.score(bfeat, blabl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setGammas = np.linspace(0.005,10.0,50)\n",
    "\n",
    "accuracies = []\n",
    "for gamma in setGammas:\n",
    "    clf = SVC( C = 10., gamma = gamma )\n",
    "    accuracies.append( clf.fit( bfeat, blabl ).score(bfeat, blabl) )\n",
    "\n",
    "plt.plot(setGammas, accuracies)\n",
    "\n",
    "plt.ylabel(r'Accuracy')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.savefig('SVM_accuracyvsgamma.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 10., gamma = 10 )\n",
    "clf.fit( bfeat, blabl )\n",
    "plotContours(clf, 'SVC_decbound_bdataset1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C = 10., gamma = 1 )\n",
    "clf.fit( bfeat, blabl )\n",
    "plotContours(clf, 'SVC_decbound_bdataset2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
