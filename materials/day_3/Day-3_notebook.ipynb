{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "Collaboratory workshop, 02/22/2018\n",
    "\n",
    "This is a notebook developed throughout the third day of the Collaboratory Workshop, Machine Learning with Python. For more information, go to the workshop home page:\n",
    "\n",
    "https://github.com/QCB-Collaboratory/W17.MachineLearning/wiki/Day-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started creating a simple linear dataset using NumPy's random function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 100     # Defining the number of samples\n",
    "\n",
    "linearCoef = 0.5     # This is the correct linear coeficient\n",
    "Intercept  = 2.2     # This is the correct intercept parameter\n",
    "\n",
    "X = np.random.random( numSamples )*10.0     # Randomly sampling X-points.\n",
    "e = np.random.random( numSamples ) - 0.5    # Noise\n",
    "\n",
    "print(\"Min of X: \", X.min())\n",
    "print(\"Max of X: \", X.max())\n",
    "print(\"Average of the error component: \", e.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = linearCoef*X + Intercept + e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'o', color=(0.2,0.6,1.0))\n",
    "\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the linear model \n",
    "$$ Y = \\beta X + \\gamma $$\n",
    "If everything works out, we should expect $\\beta \\approx 0.5$ and $\\gamma \\approx 2.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# features has shape (100,1), while X has shape (100,)\n",
    "X = X.reshape((numSamples,1))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit( X_train , Y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = np.linspace(0,10,100)\n",
    "y_array = model.predict( x_array.reshape((100,1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train, Y_train, 'o', color=(0.2,0.6,1.0))\n",
    "plt.plot(x_array, y_array, 'r-', linewidth=3.)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient: \", model.coef_ )\n",
    "print(\"Intercept:   \", model.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a 2D feature space with $X$ and $X^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(linearCoef*X + 0.15*X**2 + Intercept + e).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.reshape((len(e),1))\n",
    "Y = linearCoef*X + 0.15*X**2 + Intercept + e\n",
    "\n",
    "plt.plot(X, Y, 'o', color=(0.2,0.6,1.0))\n",
    "\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.33)\n",
    "\n",
    "features = np.zeros( (len(X_train),2) )\n",
    "features[:,0] = X_train[:,0]\n",
    "features[:,1] = X_train[:,0]**2\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit( features , Y_train )\n",
    "\n",
    "print(\"Coefs: \",model.coef_)\n",
    "print(\"Intercept: \",model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = np.linspace(0,10,100)\n",
    "y_array =  (x_array * model.coef_[0,0] + x_array**2*model.coef_[0,1] \n",
    "                + model.intercept_)\n",
    "\n",
    "plt.plot(X, Y, 'o', color=(0.2,0.6,1.0))\n",
    "plt.plot(x_array, y_array, 'r-', linewidth=3.)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second dataset\n",
    "\n",
    "Let's explore a more interesting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('Regression_Exercise_dataset.dat')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_origin = data[:,0]    # all rows, first column\n",
    "X_origin = data[:,1]   # all rows, second column\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(\n",
    "        X_origin,Y_origin,test_size=0.2,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets store in the next array the average value of the coefficeints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "degrees = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit( X , Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According the error message above, the issue is probably with the shape of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( X.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we extensively discussed on Day 2, the features should be organized in an array with shape ```(<Num Samples>, <Num Features>)```. In this particular case, we have only a single feature, so ```Num Features```=1. This is why we will update the shape of X using numpy's reshape function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape( (X.shape[0], 1 )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( X.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "degrees = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit( X , Y )\n",
    "\n",
    "degrees.append(1) #One degree\n",
    "coefs.append( np.abs(model.coef_).mean() )\n",
    "\n",
    "x_array = np.linspace(0,1,100)\n",
    "x_array = x_array.reshape((len(x_array),1))\n",
    "y_array = model.predict(x_array)\n",
    "\n",
    "plt.plot(X, Y, 'bo')\n",
    "plt.plot(x_array, y_array, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a quadratice term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = np.c_[ X, X**2 ]\n",
    "print( X_poly.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit( X_poly , Y )\n",
    "\n",
    "degrees.append(2) #Second degree\n",
    "coefs.append( np.abs(model.coef_).mean() )\n",
    "\n",
    "x_array = np.linspace(0,1,100)\n",
    "x_array_poly = np.c_[ x_array, x_array**2 ]\n",
    "y_array = model.predict(x_array_poly)\n",
    "\n",
    "plt.plot(X, Y, 'bo')\n",
    "plt.plot(x_array, y_array, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that create the 2D array of features for any degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPoly(myArray,degree):\n",
    "    \n",
    "    result = np.zeros((myArray.shape[0],degree))\n",
    "    for j in range(degree):\n",
    "        result[:,j] = myArray.ravel()**(j+1)\n",
    "    return result\n",
    "\n",
    "X_poly = getPoly(X,degree=5)\n",
    "print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a fifth-degree polynomial. Of course, adding terms by hand is not very efficient, but we can construct the ```X_poly``` array for an arbitrary polynomial by using the following snippet of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "X_poly = getPoly(X,degree = d)\n",
    "x_array_poly = getPoly(x_array,degree = d)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit( X_poly , Y )\n",
    "\n",
    "degrees.append(d) #d-th degree\n",
    "coefs.append( np.abs(model.coef_).mean() )\n",
    "\n",
    "y_array = model.predict(x_array_poly)\n",
    "\n",
    "plt.plot(X, Y, 'bo')\n",
    "plt.plot(x_array, y_array, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 19\n",
    "X_poly = getPoly(X,degree = d)\n",
    "x_array_poly = getPoly(x_array,degree = d)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit( X_poly , Y )\n",
    "\n",
    "degrees.append(d) #d-th degree\n",
    "coefs.append( np.abs(model.coef_).mean() )\n",
    "\n",
    "y_array = model.predict(x_array_poly)\n",
    "\n",
    "plt.plot(X, Y, 'bo')\n",
    "plt.plot(x_array, y_array, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(degrees)\n",
    "print(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(degrees, coefs,'bo--', markersize=8)\n",
    "\n",
    "plt.ylabel('Avg. coefficient')\n",
    "plt.xlabel('Degree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring meta-parameter regularization using the Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge( alpha = 1.0 )\n",
    "model.fit( X_poly , Y )\n",
    "\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(x_array, model.predict(x_array_poly), 'r-')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge( alpha = 0.001 )\n",
    "model.fit( X_poly , Y )\n",
    "\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(x_array, model.predict(x_array_poly), 'r-')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "Let's explore the iris dataset using the K-means clustering algorithm. We first import the data using ```sklearn.datasets```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "print( iris_data.data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's visualize the dataset. You can change the features being visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( iris_data.data[:,1], iris_data.data[:,2], 'o' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( iris_data.data[:,1], iris_data.data[:,2], 'o', \n",
    "            color=(0.2,0.5,1.0), markersize=4 )\n",
    "\n",
    "plt.xlabel('Sepal width (cm)')\n",
    "plt.ylabel('Petal width (cm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a KMeans model and apply to the iris dataset. Because we know there are three different species of plants in this dataset, let's make an educated guess and use $K=3$ (i.e., we will set the parameter ```n_clusters``` to 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans( n_clusters=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit( iris_data.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the K-means method is applied to the dataset, we can then get the ID of the clusters to which each of the samples is predicted to belong to by using the method ```predict```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans.predict( iris_data.data )\n",
    "print( \"Shape: \", clusters.shape )\n",
    "print( \"Cluster IDs: \", clusters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the resulting clustering by color-coding each cluster ID. This is very similar to how we color-coded different classes in classification datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index0 = clusters == 0\n",
    "index1 = clusters == 1\n",
    "index2 = clusters == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( iris_data.data[index0,1], iris_data.data[index0,2], \n",
    "            'o', color='b', markersize=4 )\n",
    "plt.plot( iris_data.data[index1,1], iris_data.data[index1,2], \n",
    "            'o', color='k', markersize=4 )\n",
    "plt.plot( iris_data.data[index2,1], iris_data.data[index2,2], \n",
    "            'o', color='r', markersize=4 )\n",
    "\n",
    "plt.xlabel('Sepal width (cm)')\n",
    "plt.ylabel('Petal width (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try $K=4$ and inspect what comes out differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans( n_clusters=4 )\n",
    "kmeans.fit( iris_data.data )\n",
    "clusters = kmeans.predict( iris_data.data )\n",
    "\n",
    "index0 = clusters == 0\n",
    "index1 = clusters == 1\n",
    "index2 = clusters == 2\n",
    "index3 = clusters == 3\n",
    "\n",
    "plt.plot( iris_data.data[index0,1], iris_data.data[index0,2], \n",
    "            'o', color='b', markersize=4 )\n",
    "plt.plot( iris_data.data[index1,1], iris_data.data[index1,2], \n",
    "            'o', color='k', markersize=4 )\n",
    "plt.plot( iris_data.data[index2,1], iris_data.data[index2,2], \n",
    "            'o', color='r', markersize=4 )\n",
    "plt.plot( iris_data.data[index3,1], iris_data.data[index3,2], \n",
    "            'o', color='g', markersize=4 )\n",
    "\n",
    "plt.xlabel('Sepal width (cm)')\n",
    "plt.ylabel('Petal width (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 samples with 3 features\n",
    "X = np.random.random( (200, 3) )   \n",
    "X[:,2] = X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\n breaks the line\n",
    "print(\"Covariance matrix:\\n \", np.cov(X.T))  \n",
    "plt.matshow(np.cov(X.T))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = pca.transform(X)\n",
    "print(np.cov(X_transform.T))\n",
    "plt.matshow(np.cov(X_transform.T))\n",
    "plt.savefig('CovMatrix_example_PCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using PCA on the Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bcancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(\n",
    "        bcancer.data,bcancer.target,test_size=0.2,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit( X )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the importance of each Principal Component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( pca.explained_variance_ratio_, 'o--' )\n",
    "plt.ylabel( 'Explained Variance' )\n",
    "plt.xlabel( 'Principal Component #' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize the decay is using log-scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( pca.explained_variance_ratio_, 'o--' )\n",
    "plt.ylabel( 'Explained Variance' )\n",
    "plt.yscale('log')\n",
    "plt.xlabel( 'Principal Component #' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCAs = pca.transform( X )\n",
    "X_PCAs = X_PCAs[:,:2]\n",
    "\n",
    "index0 = (Y == 0)\n",
    "index1 = (Y == 1)\n",
    "\n",
    "plt.plot( X_PCAs[index0,0], \n",
    "         X_PCAs[index0,1], 's', color='r' )\n",
    "plt.plot( X_PCAs[index1,0], \n",
    "         X_PCAs[index1,1], 'o', color='b' )\n",
    "\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100, max_depth = 5)\n",
    "clf.fit(X_PCAs,Y)\n",
    "\n",
    "\n",
    "X_test_PCAs = pca.transform( X_test )\n",
    "clf.score(X_test_PCAs[:,:2],Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "Here are the solutions of the one (out of two) exercise left for you to try after the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2 - Interpreting \"principal axis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 100     # Defining the number of samples\n",
    "\n",
    "linearCoef = 0.5     # This is the correct linear coeficient\n",
    "Intercept  = 2.2     # This is the correct intercept parameter\n",
    "\n",
    "X = np.random.random( numSamples )*10.0     # Randomly sampling X-points.\n",
    "e = np.random.random( numSamples ) - 0.5    # Noise\n",
    "Y = linearCoef*X + Intercept + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array( [X, Y] ).T\n",
    "np.cov(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y,'o')\n",
    "plt.plot([0,pca.components_[0,0]],[0,pca.components_[0,1]],'r-')\n",
    "plt.plot([0,pca.components_[1,0]],[0,pca.components_[1,1]],'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = pca.transform( data )\n",
    "np.cov(X_transform.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
